services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    # Uncomment the deploy section below if you have an NVIDIA GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

    # Health check to ensure Ollama is running
    # This may be causing a performance hit, but the client needs to know when Ollama is ready
    # If performance is an issue, consider removing this health check and run client manually
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Client for local development with Ollama
  client-local:
    build: .
    container_name: client-local
    depends_on:
      ollama:
        condition: service_healthy
    env_file:
      - .env.local
    command:
      - --input
      - /app/files/input
      - --output
      - /app/files/output
      - --local
      - --model
      - mistral # Default model for local development - can be changed
    volumes:
      - ./files/input:/app/files/input
      - ./files/output:/app/files/output

volumes:
  ollama_models:
